{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37283c0b",
   "metadata": {},
   "source": [
    "<a name='top'></a>\n",
    "Javier Lopez<br>\n",
    "Student ID: 000697446\n",
    "# Prepare NYC Rideshare, Weather, Covid, and Vaccination Data\n",
    "## Feb 2019-Mar 2022\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb366850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778988d0",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a0fd94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    process_tic = time.perf_counter()\n",
    "    clean_tripdata()\n",
    "    resample_tripdata()\n",
    "    concat_dfs()\n",
    "    clean_taxi_zones()\n",
    "    uber = pd.read_parquet('data/clean_data/uber_taxi_zone_tripdata.parquet')\n",
    "    uber.name = 'uber'\n",
    "    lyft = pd.read_parquet('data/clean_data/lyft_taxi_zone_tripdata.parquet')\n",
    "    lyft.name = 'lyft'\n",
    "    [get_borough_tripdata(df) for df in [uber, lyft]]\n",
    "    midprocess_toc = time.perf_counter()\n",
    "    print('Rideshare data cleaning completed in {:.2f} minutes' .format((midprocess_toc - process_tic)/60))\n",
    "    clean_weather_data()\n",
    "    clean_covid_data()\n",
    "    clean_vax_data()\n",
    "    process_toc = time.perf_counter()\n",
    "    print('All data cleaning completed in {:.2f} minutes' .format((process_toc - process_tic)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d3d37",
   "metadata": {},
   "source": [
    "## Clean Rideshare Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf32e1",
   "metadata": {},
   "source": [
    "### By Taxi Zone\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f37b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, df_name):\n",
    "    # Set index to pickup datetime\n",
    "    df.set_index('pickup_datetime', drop=True, inplace=True)\n",
    "    df.index.name = None\n",
    "\n",
    "    # Make all columns lowercase\n",
    "    df.columns = [df[col].name.lower() for col in df.columns]\n",
    "\n",
    "    # Keep only study columns\n",
    "    df = df[['hvfhs_license_num', 'pulocationid']]\n",
    "\n",
    "    # Split by provider\n",
    "    uber = df[['pulocationid']][df.hvfhs_license_num == 'HV0003']\n",
    "    lyft = df[['pulocationid']][df.hvfhs_license_num == 'HV0005']\n",
    "    del df\n",
    "\n",
    "    # Create dummy variables for taxi zones\n",
    "    uber.pulocationid, lyft.pulocationid = [df.pulocationid.astype(str) for df in [uber, lyft]]\n",
    "    uber, lyft = [pd.concat([df, pd.get_dummies(df.pulocationid)], axis=1) for df in [uber, lyft]]\n",
    "    \n",
    "    # Drop pulocationid column\n",
    "    uber, lyft = [df.drop('pulocationid', axis=1) for df in [uber, lyft]]\n",
    "    \n",
    "    # Rename columns\n",
    "    uber.columns = ['zone_' + col for col in uber.columns]\n",
    "    lyft.columns = ['zone_' + col for col in lyft.columns]\n",
    "\n",
    "    # Drop 'EWR' and 'Unknown' taxi zones\n",
    "    drop_zones = ['zone_1', 'zone_264', 'zone_265']\n",
    "    for zone in drop_zones:\n",
    "        for df in [uber, lyft]:\n",
    "            if zone in df.columns:\n",
    "                df.drop(zone, axis=1, inplace=True)\n",
    "\n",
    "    uber.to_parquet('data/rideshare_data/clean_rideshare_data/uber_{}.parquet' .format(df_name))\n",
    "    lyft.to_parquet('data/rideshare_data/clean_rideshare_data/lyft_{}.parquet' .format(df_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea3bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tripdata files\n",
    "def clean_tripdata():\n",
    "    load_clean_tic = time.perf_counter()\n",
    "    for root, dirs, files in os.walk('./data/rideshare_data', topdown=False):\n",
    "        for name in files:\n",
    "            if name != '.DS_Store':\n",
    "                filename = name[6:22].replace('-', '_')\n",
    "                # Load file\n",
    "                load_tic = time.perf_counter()\n",
    "                df = pd.read_parquet(os.path.join(root, name))\n",
    "                load_toc = time.perf_counter()\n",
    "                print(filename, 'loading completed in {:.2f} minutes' .format((load_toc - load_tic)/60))\n",
    "                # Clean file\n",
    "                clean_tic = time.perf_counter()\n",
    "                clean_df(df, filename)\n",
    "                clean_toc = time.perf_counter()\n",
    "                print(filename, 'cleaning completed in {:.2f} minutes' .format((clean_toc - clean_tic)/60))\n",
    "    load_clean_toc = time.perf_counter()\n",
    "    print('All file cleaning completed in {:2f} minutes' .format((load_clean_toc - load_clean_tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adc0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_tripdata():\n",
    "    tic = time.perf_counter()\n",
    "    for root, dirs, files in os.walk('./data/rideshare_data/clean_rideshare_data', topdown=False):\n",
    "        for name in files:\n",
    "            if name != '.DS_Store':\n",
    "                filename = name[:-8]\n",
    "                # Load file\n",
    "                load_tic = time.perf_counter()\n",
    "                df = pd.read_parquet(os.path.join(root, name))\n",
    "                load_toc = time.perf_counter()\n",
    "                print(filename, 'loading completed in {:.2f} minutes' .format((load_toc - load_tic)/60))\n",
    "                # Resample by day\n",
    "                resample_tic = time.perf_counter()\n",
    "                df.index = pd.to_datetime(df.index).normalize()\n",
    "                df = df.resample('D').agg(np.sum)\n",
    "                df.to_parquet('data/rideshare_data/resampled_rideshare_data/{}.parquet' .format(filename))\n",
    "                resample_toc = time.perf_counter()\n",
    "                print(filename, 'resampling completed in {:.2f} minutes' .format((resample_toc - resample_tic)/60))\n",
    "    toc = time.perf_counter()\n",
    "    print('All resampling completed in {:2f} minutes' .format((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5e8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to concatenate dataframes\n",
    "def concat_dfs():\n",
    "    tic = time.perf_counter()\n",
    "    uber = pd.DataFrame()\n",
    "    lyft = pd.DataFrame()\n",
    "    for root, dirs, files in os.walk('./data/rideshare_data/resampled_rideshare_data', topdown=False):\n",
    "        for name in files:\n",
    "            if name != '.DS_Store':\n",
    "                # Load file\n",
    "                df = pd.read_parquet(os.path.join(root, name))\n",
    "                if 'uber' in name:\n",
    "                    uber = pd.concat([uber, df])\n",
    "                    print(name, 'loaded into uber dataframe')\n",
    "                if 'lyft' in name:\n",
    "                    lyft = pd.concat([lyft, df])\n",
    "                    print(name, 'loaded into lyft dataframe')\n",
    "    uber.sort_index(inplace=True)\n",
    "    lyft.sort_index(inplace=True)\n",
    "    uber.to_parquet('data/clean_data/uber_taxi_zone_tripdata.parquet')\n",
    "    lyft.to_parquet('data/clean_data/lyft_taxi_zone_tripdata.parquet')\n",
    "    toc = time.perf_counter()\n",
    "    print('Concatenation completed in {:.2f} minutes' .format((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58e7b1",
   "metadata": {},
   "source": [
    "### By Borough\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba38c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_zones():\n",
    "    # Load taxi zones\n",
    "    taxi_zones = pd.read_csv('data/taxi_zones.csv')\n",
    "    \n",
    "    # Make columns lowercase\n",
    "    taxi_zones.columns = [taxi_zones[col].name.lower() for col in taxi_zones]\n",
    "    \n",
    "    # Select necessary columns\n",
    "    taxi_zones = taxi_zones[['locationid', 'borough']]\n",
    "    \n",
    "    taxi_zones.to_csv('data/clean_data/taxi_zones_clean.csv', index=False)\n",
    "    print('File cleaning completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf83e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borough_tripdata(df):\n",
    "    # Load taxi zones\n",
    "    taxi_zones = pd.read_csv('data/clean_data/taxi_zones_clean.csv')\n",
    "    \n",
    "    # List boroughs\n",
    "    boroughs = ['Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island']\n",
    "    \n",
    "    # Get taxi zones for each borough\n",
    "    zone_dict = {}\n",
    "    for borough in boroughs:\n",
    "        zone_dict[borough] = taxi_zones.locationid.values[taxi_zones.borough == borough]\n",
    "    \n",
    "    # Make dataframe for each borough\n",
    "    borough_dfs = {}\n",
    "    for borough in boroughs:\n",
    "        borough_df = pd.DataFrame()\n",
    "        for num in zone_dict[borough]:\n",
    "            for col in df.columns:\n",
    "                if col == 'zone_' + str(num):\n",
    "                    borough_df[col] = df[col]\n",
    "        borough_dfs[borough] = borough_df\n",
    "    \n",
    "    # Make dataframe of borough aggregation\n",
    "    borough_tripdata = pd.DataFrame(index=df.index)\n",
    "    for key, value in borough_dfs.items():\n",
    "        borough_tripdata[key.lower().replace(' ', '_')] = [value.loc[value.index[i]].sum() for i in range(len(value))]\n",
    "    \n",
    "    # Add nyc total column\n",
    "    borough_tripdata['nyc'] = [\n",
    "        borough_tripdata.loc[borough_tripdata.index[i]].sum() for i in range(len(borough_tripdata))]\n",
    "    \n",
    "    borough_tripdata.to_parquet('data/clean_data/{}_borough_tripdata.parquet' .format(df.name))\n",
    "    print('Aggregation completed for', df.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58da33",
   "metadata": {},
   "source": [
    "## Clean Weather Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d3fc0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_weather_data():\n",
    "    # Load file\n",
    "    df = pd.read_csv('data/weather.csv')\n",
    "\n",
    "    # Set index to datetime\n",
    "    df.set_index(pd.to_datetime(df.DATE), inplace=True)\n",
    "    df.drop('DATE', axis=1, inplace=True)\n",
    "    df.index.name = None\n",
    "\n",
    "    # Make columns lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Select necessary columns\n",
    "    df = df[[ 'prcp', 'snow', 'tmax', 'tmin']]\n",
    "    df.columns = ['precipitation', 'snowfall', 'max_temp', 'min_temp']\n",
    "\n",
    "    # Get average temperature\n",
    "    df['temp'] = (df.max_temp + df.min_temp) / 2\n",
    "\n",
    "    # Select necessary columns\n",
    "    df = df[['temp', 'precipitation', 'snowfall']]\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['temp', 'prec', 'snow']\n",
    "    \n",
    "    # Select study dates\n",
    "    df = df['2019-02-01':'2022-03-31']\n",
    "    \n",
    "    df.to_parquet('data/clean_data/weather_data_clean.parquet')\n",
    "    print('Weather data cleaning completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87908e2",
   "metadata": {},
   "source": [
    "## Clean Covid Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f5862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_covid_data():\n",
    "    # Load file\n",
    "    df = pd.read_csv('data/covid_boroughs_case_hosp_death.csv')\n",
    "    \n",
    "    # Set index to datetime\n",
    "    df.set_index(pd.to_datetime(df.DATE_OF_INTEREST), inplace=True)\n",
    "    df.drop('DATE_OF_INTEREST', axis=1, inplace=True)\n",
    "    df.index.name = None\n",
    "    \n",
    "    # Make columns lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "    # Re-encode column names\n",
    "    boroughs = {'bx':'bronx','bk':'brooklyn','mn':'manhattan','qn':'queens','si':'staten_island'}\n",
    "    colnames = []\n",
    "    to_drop = []\n",
    "    for col in df.columns:\n",
    "        for key, value in boroughs.items():\n",
    "            if key + '_' in col:\n",
    "                col = col.replace(key + '_', value + '_')\n",
    "        if '7day_avg' not in col or 'all' in col:\n",
    "            to_drop.append(col)\n",
    "        colnames.append(col)\n",
    "    df.columns = colnames\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    df.columns.values[:3] = ['nyc_' + col for col in df.columns[:3]]\n",
    "    df.columns = [col[:-15] for col in df.columns]\n",
    "    df.columns.values[1] = df.columns.values[1][:7]+'pitalized'\n",
    "    \n",
    "    # Select study dates\n",
    "    df = df['2019-02-01':'2022-03-31']\n",
    "    \n",
    "    df.to_parquet('data/clean_data/covid_data_clean.parquet')\n",
    "    print('Covid data cleaning completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b425b",
   "metadata": {},
   "source": [
    "## Clean Vaccination Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f556c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vax_data():\n",
    "    # Load file\n",
    "    df = pd.read_csv('data/Covid-19_Vaccinations.csv')\n",
    "\n",
    "    # Set index to datetime\n",
    "    df.set_index(pd.to_datetime(df.Date), inplace=True)\n",
    "    df.drop('Date', axis=1, inplace=True)\n",
    "    df.index.name = None\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Select necessary columns\n",
    "    df['vaccinated'] = df['Administered_Dose1_Recip'].shift(-1)-df['Administered_Dose1_Recip']\n",
    "    df = df[['vaccinated']]\n",
    "    df = df['2021-04-16':'2022-03-31']\n",
    "\n",
    "    df.to_parquet('data/clean_data/vaccination_data_clean.parquet')\n",
    "    print('Vaccination data cleaning completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546c9f1",
   "metadata": {},
   "source": [
    "## Get Temporal Phase Dummies\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b92c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/clean_data/'\n",
    "uber = pd.read_parquet(root + 'uber_borough_tripdata.parquet')\n",
    "lyft = pd.read_parquet(root + 'lyft_borough_tripdata.parquet')\n",
    "weather = pd.read_parquet(root + 'weather_data_clean.parquet')\n",
    "covid = pd.read_parquet(root + 'covid_data_clean.parquet')\n",
    "vax = pd.read_parquet(root + 'vaccination_data_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4092daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = pd.DataFrame(index=uber.index, columns=['phase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10584680",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases.phase['2019-02-01':'2020-03-10'] = ['phase_0']*len(phases.phase['2019-02-01':'2020-03-10'])\n",
    "phases.phase['2020-03-11':'2020-05-19'] = ['phase_1']*len(phases.phase['2020-03-11':'2020-05-19'])\n",
    "phases.phase['2020-05-20':'2021-04-15'] = ['phase_2']*len(phases.phase['2020-05-20':'2021-04-15'])\n",
    "phases.phase['2021-04-16':] = ['phase_3']*len(phases.phase['2021-04-16':])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ef4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = pd.get_dummies(phases.phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d5dbb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase_0</th>\n",
       "      <th>phase_1</th>\n",
       "      <th>phase_2</th>\n",
       "      <th>phase_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            phase_0  phase_1  phase_2  phase_3\n",
       "2019-02-01        1        0        0        0\n",
       "2019-02-02        1        0        0        0\n",
       "2019-02-03        1        0        0        0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4a26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1155 entries, 2019-02-01 to 2022-03-31\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   phase_0  1155 non-null   uint8\n",
      " 1   phase_1  1155 non-null   uint8\n",
      " 2   phase_2  1155 non-null   uint8\n",
      " 3   phase_3  1155 non-null   uint8\n",
      "dtypes: uint8(4)\n",
      "memory usage: 45.8 KB\n"
     ]
    }
   ],
   "source": [
    "phases.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f28a75",
   "metadata": {},
   "source": [
    "##  Condense Data by Borough\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db05d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vars(borough):\n",
    "    df = pd.concat([uber[[borough]], lyft[[borough]]], axis=1)\n",
    "    df.columns = ['uber', 'lyft']\n",
    "    df = pd.concat([df, weather], axis=1)\n",
    "    cols = [col for col in covid.columns if borough in col]\n",
    "    df = pd.concat([df, covid[cols]], axis=1).fillna(0)\n",
    "    df = pd.concat([df, vax], axis=1).fillna(0)\n",
    "    df = pd.concat([df, phases], axis=1).fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ea707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronx, brooklyn, manhattan, queens, staten_island, nyc = [concat_vars(borough) for borough in uber.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3da9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [\n",
    "    uber.columns.values,\n",
    "    ['uber','lyft','temp','prec','snow','case','hospitalized','death','vaccination',\n",
    "     'phase_0', 'phase_1', 'phase_2', 'phase_3']\n",
    "]\n",
    "columns = pd.MultiIndex.from_product(iterables)\n",
    "data = pd.DataFrame(index=uber.index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d69fd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for borough in uber.columns:\n",
    "    data[borough] = globals()[borough].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "153b0bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">bronx</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">nyc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>uber</th>\n",
       "      <th>lyft</th>\n",
       "      <th>temp</th>\n",
       "      <th>prec</th>\n",
       "      <th>snow</th>\n",
       "      <th>case</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>death</th>\n",
       "      <th>vaccination</th>\n",
       "      <th>phase_0</th>\n",
       "      <th>...</th>\n",
       "      <th>prec</th>\n",
       "      <th>snow</th>\n",
       "      <th>case</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>death</th>\n",
       "      <th>vaccination</th>\n",
       "      <th>phase_0</th>\n",
       "      <th>phase_1</th>\n",
       "      <th>phase_2</th>\n",
       "      <th>phase_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>73444.0</td>\n",
       "      <td>13319.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>73752.0</td>\n",
       "      <td>9929.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>67072.0</td>\n",
       "      <td>9087.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>59847.0</td>\n",
       "      <td>9852.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>51170.0</td>\n",
       "      <td>9239.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              bronx                                                   \\\n",
       "               uber     lyft  temp prec snow case hospitalized death   \n",
       "2019-02-01  73444.0  13319.0  16.0  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-02  73752.0   9929.0  25.0  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-03  67072.0   9087.0  43.0  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-04  59847.0   9852.0  51.0  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-05  51170.0   9239.0  54.5  0.0  0.0  0.0          0.0   0.0   \n",
       "\n",
       "                                ...  nyc                               \\\n",
       "           vaccination phase_0  ... prec snow case hospitalized death   \n",
       "2019-02-01         0.0     1.0  ...  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-02         0.0     1.0  ...  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-03         0.0     1.0  ...  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-04         0.0     1.0  ...  0.0  0.0  0.0          0.0   0.0   \n",
       "2019-02-05         0.0     1.0  ...  0.0  0.0  0.0          0.0   0.0   \n",
       "\n",
       "                                                        \n",
       "           vaccination phase_0 phase_1 phase_2 phase_3  \n",
       "2019-02-01         0.0     1.0     0.0     0.0     0.0  \n",
       "2019-02-02         0.0     1.0     0.0     0.0     0.0  \n",
       "2019-02-03         0.0     1.0     0.0     0.0     0.0  \n",
       "2019-02-04         0.0     1.0     0.0     0.0     0.0  \n",
       "2019-02-05         0.0     1.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d0226b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1155 entries, 2019-02-01 to 2022-03-31\n",
      "Data columns (total 78 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   (bronx, uber)                  1155 non-null   float64\n",
      " 1   (bronx, lyft)                  1155 non-null   float64\n",
      " 2   (bronx, temp)                  1155 non-null   float64\n",
      " 3   (bronx, prec)                  1155 non-null   float64\n",
      " 4   (bronx, snow)                  1155 non-null   float64\n",
      " 5   (bronx, case)                  1155 non-null   float64\n",
      " 6   (bronx, hospitalized)          1155 non-null   float64\n",
      " 7   (bronx, death)                 1155 non-null   float64\n",
      " 8   (bronx, vaccination)           1155 non-null   float64\n",
      " 9   (bronx, phase_0)               1155 non-null   float64\n",
      " 10  (bronx, phase_1)               1155 non-null   float64\n",
      " 11  (bronx, phase_2)               1155 non-null   float64\n",
      " 12  (bronx, phase_3)               1155 non-null   float64\n",
      " 13  (brooklyn, uber)               1155 non-null   float64\n",
      " 14  (brooklyn, lyft)               1155 non-null   float64\n",
      " 15  (brooklyn, temp)               1155 non-null   float64\n",
      " 16  (brooklyn, prec)               1155 non-null   float64\n",
      " 17  (brooklyn, snow)               1155 non-null   float64\n",
      " 18  (brooklyn, case)               1155 non-null   float64\n",
      " 19  (brooklyn, hospitalized)       1155 non-null   float64\n",
      " 20  (brooklyn, death)              1155 non-null   float64\n",
      " 21  (brooklyn, vaccination)        1155 non-null   float64\n",
      " 22  (brooklyn, phase_0)            1155 non-null   float64\n",
      " 23  (brooklyn, phase_1)            1155 non-null   float64\n",
      " 24  (brooklyn, phase_2)            1155 non-null   float64\n",
      " 25  (brooklyn, phase_3)            1155 non-null   float64\n",
      " 26  (manhattan, uber)              1155 non-null   float64\n",
      " 27  (manhattan, lyft)              1155 non-null   float64\n",
      " 28  (manhattan, temp)              1155 non-null   float64\n",
      " 29  (manhattan, prec)              1155 non-null   float64\n",
      " 30  (manhattan, snow)              1155 non-null   float64\n",
      " 31  (manhattan, case)              1155 non-null   float64\n",
      " 32  (manhattan, hospitalized)      1155 non-null   float64\n",
      " 33  (manhattan, death)             1155 non-null   float64\n",
      " 34  (manhattan, vaccination)       1155 non-null   float64\n",
      " 35  (manhattan, phase_0)           1155 non-null   float64\n",
      " 36  (manhattan, phase_1)           1155 non-null   float64\n",
      " 37  (manhattan, phase_2)           1155 non-null   float64\n",
      " 38  (manhattan, phase_3)           1155 non-null   float64\n",
      " 39  (queens, uber)                 1155 non-null   float64\n",
      " 40  (queens, lyft)                 1155 non-null   float64\n",
      " 41  (queens, temp)                 1155 non-null   float64\n",
      " 42  (queens, prec)                 1155 non-null   float64\n",
      " 43  (queens, snow)                 1155 non-null   float64\n",
      " 44  (queens, case)                 1155 non-null   float64\n",
      " 45  (queens, hospitalized)         1155 non-null   float64\n",
      " 46  (queens, death)                1155 non-null   float64\n",
      " 47  (queens, vaccination)          1155 non-null   float64\n",
      " 48  (queens, phase_0)              1155 non-null   float64\n",
      " 49  (queens, phase_1)              1155 non-null   float64\n",
      " 50  (queens, phase_2)              1155 non-null   float64\n",
      " 51  (queens, phase_3)              1155 non-null   float64\n",
      " 52  (staten_island, uber)          1155 non-null   float64\n",
      " 53  (staten_island, lyft)          1155 non-null   float64\n",
      " 54  (staten_island, temp)          1155 non-null   float64\n",
      " 55  (staten_island, prec)          1155 non-null   float64\n",
      " 56  (staten_island, snow)          1155 non-null   float64\n",
      " 57  (staten_island, case)          1155 non-null   float64\n",
      " 58  (staten_island, hospitalized)  1155 non-null   float64\n",
      " 59  (staten_island, death)         1155 non-null   float64\n",
      " 60  (staten_island, vaccination)   1155 non-null   float64\n",
      " 61  (staten_island, phase_0)       1155 non-null   float64\n",
      " 62  (staten_island, phase_1)       1155 non-null   float64\n",
      " 63  (staten_island, phase_2)       1155 non-null   float64\n",
      " 64  (staten_island, phase_3)       1155 non-null   float64\n",
      " 65  (nyc, uber)                    1155 non-null   float64\n",
      " 66  (nyc, lyft)                    1155 non-null   float64\n",
      " 67  (nyc, temp)                    1155 non-null   float64\n",
      " 68  (nyc, prec)                    1155 non-null   float64\n",
      " 69  (nyc, snow)                    1155 non-null   float64\n",
      " 70  (nyc, case)                    1155 non-null   float64\n",
      " 71  (nyc, hospitalized)            1155 non-null   float64\n",
      " 72  (nyc, death)                   1155 non-null   float64\n",
      " 73  (nyc, vaccination)             1155 non-null   float64\n",
      " 74  (nyc, phase_0)                 1155 non-null   float64\n",
      " 75  (nyc, phase_1)                 1155 non-null   float64\n",
      " 76  (nyc, phase_2)                 1155 non-null   float64\n",
      " 77  (nyc, phase_3)                 1155 non-null   float64\n",
      "dtypes: float64(78)\n",
      "memory usage: 745.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd08f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('data/clean_data/model_dataset.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
